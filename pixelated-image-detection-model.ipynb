{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8662624,"sourceType":"datasetVersion","datasetId":5190395},{"sourceId":8812753,"sourceType":"datasetVersion","datasetId":5301083}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Define constants\nIMG_HEIGHT, IMG_WIDTH = 128, 128\nBATCH_SIZE = 32\nEPOCHS = 10  # Increased number of epochs\n\n# Define paths to your dataset folders\ntrain_dir = '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing/'\ntrain_pixelated_dir = os.path.join(train_dir, 'Pixelated')\ntrain_original_dir = os.path.join(train_dir, 'Original')\n\n# Function to split data into train and validation directories\ndef split_data(original_dir, pixelated_dir, train_dir, test_dir, split_ratio):\n    os.makedirs(os.path.join(train_dir, 'Original'), exist_ok=True)\n    os.makedirs(os.path.join(train_dir, 'Pixelated'), exist_ok=True)\n    os.makedirs(os.path.join(test_dir, 'Original'), exist_ok=True)\n    os.makedirs(os.path.join(test_dir, 'Pixelated'), exist_ok=True)\n    \n    original_files = os.listdir(original_dir)\n    pixelated_files = os.listdir(pixelated_dir)\n    paired_files = [(file, file) for file in original_files if file in pixelated_files]\n    np.random.shuffle(paired_files)\n    split_index = int(len(paired_files) * split_ratio)\n    train_files = paired_files[:split_index]\n    test_files = paired_files[split_index:]\n    \n    for original_file, pixelated_file in train_files:\n        shutil.copy(os.path.join(original_dir, original_file), os.path.join(train_dir, 'Original', original_file))\n        shutil.copy(os.path.join(pixelated_dir, pixelated_file), os.path.join(train_dir, 'Pixelated', pixelated_file))\n        \n    for original_file, pixelated_file in test_files:\n        shutil.copy(os.path.join(original_dir, original_file), os.path.join(test_dir, 'Original', original_file))\n        shutil.copy(os.path.join(pixelated_dir, pixelated_file), os.path.join(test_dir, 'Pixelated', pixelated_file))\n\n# Split the data\nbase_dir = '/kaggle/working/'\ntrain_dir = os.path.join(base_dir, 'train')\ntest_dir = os.path.join(base_dir, 'test')\nsplit_ratio = 0.8\nsplit_data(train_original_dir, train_pixelated_dir, train_dir, test_dir, split_ratio)\n\n# ImageDataGenerator for augmentation and scaling\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,  # Reduced range\n    width_shift_range=0.1,  # Reduced range\n    height_shift_range=0.1,  # Reduced range\n    shear_range=0.1,  # Reduced range\n    zoom_range=0.1,  # Reduced range\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\n# Generate batches of augmented data\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    test_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary'\n)\n\n# Define the model architecture\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n    MaxPooling2D((2, 2)),\n    # BatchNormalization(),  # Removed for simplicity\n\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    # BatchNormalization(),  # Removed for simplicity\n\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    # BatchNormalization(),  # Removed for simplicity\n\n    Conv2D(256, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    # BatchNormalization(),  # Removed for simplicity\n\n    GlobalAveragePooling2D(),\n\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss=BinaryCrossentropy(),\n              metrics=[BinaryAccuracy(), Precision(), Recall()])\n\n# Define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # Increased patience\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\nmodel_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n)\n\n# Save the model\nmodel.save('/kaggle/working/pixelated_detection_model.keras')\n\n# Calculate and print model size\nmodel_size = os.path.getsize('/kaggle/working/pixelated_detection_model.keras') / (1024 * 1024)\nprint(f'Model Size: {model_size:.2f} MB')\n\n# Evaluate the model\ntest_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False\n)\n\nevaluation_results = model.evaluate(test_generator)\nloss, accuracy, precision, recall = evaluation_results[:4]\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')\n\n# Example of using the trained model for prediction\nsample_image_path = '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing/Original/113.png'\nsample_image = load_img(sample_image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\nsample_image = img_to_array(sample_image)\nsample_image = np.expand_dims(sample_image, axis=0) / 255.0\nprediction = model.predict(sample_image)\nif prediction[0] > 0.5:\n    print('Prediction: Pixelated')\nelse:\n    print('Prediction: Not Pixelated')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-09T16:03:15.151882Z","iopub.execute_input":"2024-07-09T16:03:15.152340Z","iopub.status.idle":"2024-07-09T16:13:29.021688Z","shell.execute_reply.started":"2024-07-09T16:03:15.152304Z","shell.execute_reply":"2024-07-09T16:13:29.020402Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 2482 images belonging to 2 classes.\nFound 1510 images belonging to 2 classes.\nEpoch 1/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 1s/step - binary_accuracy: 0.4957 - loss: 0.6966 - precision_3: 0.5071 - recall_3: 0.6026 - val_binary_accuracy: 0.6822 - val_loss: 0.6470 - val_precision_3: 0.8026 - val_recall_3: 0.4854 - learning_rate: 0.0010\nEpoch 2/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - binary_accuracy: 0.6562 - loss: 0.6693 - precision_3: 0.6250 - recall_3: 0.6667 - val_binary_accuracy: 1.0000 - val_loss: 0.5811 - val_precision_3: 1.0000 - val_recall_3: 1.0000 - learning_rate: 0.0010\nEpoch 3/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1s/step - binary_accuracy: 0.6844 - loss: 0.6390 - precision_3: 0.7720 - recall_3: 0.5235 - val_binary_accuracy: 0.6961 - val_loss: 0.5742 - val_precision_3: 0.8537 - val_recall_3: 0.4734 - learning_rate: 0.0010\nEpoch 4/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - binary_accuracy: 0.5625 - loss: 0.5945 - precision_3: 1.0000 - recall_3: 0.3000 - val_binary_accuracy: 1.0000 - val_loss: 0.3149 - val_precision_3: 1.0000 - val_recall_3: 1.0000 - learning_rate: 0.0010\nEpoch 5/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - binary_accuracy: 0.6914 - loss: 0.5874 - precision_3: 0.9035 - recall_3: 0.4474 - val_binary_accuracy: 0.6928 - val_loss: 0.5825 - val_precision_3: 0.9649 - val_recall_3: 0.4011 - learning_rate: 0.0010\nEpoch 6/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 822us/step - binary_accuracy: 0.5625 - loss: 0.6804 - precision_3: 0.8000 - recall_3: 0.2353 - val_binary_accuracy: 0.8333 - val_loss: 0.5565 - val_precision_3: 1.0000 - val_recall_3: 0.5000 - learning_rate: 0.0010\nEpoch 7/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - binary_accuracy: 0.6985 - loss: 0.5765 - precision_3: 0.9120 - recall_3: 0.4334 - val_binary_accuracy: 0.7061 - val_loss: 0.5495 - val_precision_3: 0.9286 - val_recall_3: 0.4483 - learning_rate: 0.0010\nEpoch 8/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816us/step - binary_accuracy: 0.8125 - loss: 0.4257 - precision_3: 0.9231 - recall_3: 0.7059 - val_binary_accuracy: 0.8333 - val_loss: 0.5956 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - learning_rate: 0.0010\nEpoch 9/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1s/step - binary_accuracy: 0.6917 - loss: 0.5621 - precision_3: 0.9263 - recall_3: 0.4374 - val_binary_accuracy: 0.7015 - val_loss: 0.5472 - val_precision_3: 0.8619 - val_recall_3: 0.4807 - learning_rate: 0.0010\nEpoch 10/10\n\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817us/step - binary_accuracy: 0.6562 - loss: 0.5517 - precision_3: 0.7500 - recall_3: 0.5294 - val_binary_accuracy: 0.6667 - val_loss: 0.5205 - val_precision_3: 0.5000 - val_recall_3: 0.5000 - learning_rate: 2.0000e-04\nModel Size: 4.97 MB\nFound 1510 images belonging to 2 classes.\n\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 575ms/step - binary_accuracy: 0.8510 - loss: 0.5925 - precision_3: 0.3717 - recall_3: 0.2589\nTest Accuracy: 70.33%\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\nPrediction: Not Pixelated\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}