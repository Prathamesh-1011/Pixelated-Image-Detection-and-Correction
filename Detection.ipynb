{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8662624,"sourceType":"datasetVersion","datasetId":5190395},{"sourceId":8784453,"sourceType":"datasetVersion","datasetId":5280811},{"sourceId":8812564,"sourceType":"datasetVersion","datasetId":5300961},{"sourceId":8812753,"sourceType":"datasetVersion","datasetId":5301083}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_curve, accuracy_score\nimport cv2\nimport numpy as np\nimport os\n\n# Pixelation function\ndef pixelate_image(image, scale=0.1):\n    height, width, _ = image.shape\n    small = cv2.resize(image, (int(width * scale), int(height * scale)), interpolation=cv2.INTER_LINEAR)\n    return cv2.resize(small, (width, height), interpolation=cv2.INTER_NEAREST)\n\n# Data Preparation\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rotation_range=30,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    brightness_range=[0.8, 1.2],\n    preprocessing_function=lambda x: pixelate_image(x, scale=0.1) if np.random.rand() > 0.5 else x\n)\n\ntrain_generator = datagen.flow_from_directory(\n    '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary',\n    subset='training'\n)\n\nvalidation_generator = datagen.flow_from_directory(\n    '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)\n\n# Compute class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_generator.classes),\n    y=train_generator.classes\n)\nclass_weights = {i : class_weights[i] for i in range(len(class_weights))}\n\n# Model Design\nweights_path = '/kaggle/input/models/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5'\nbase_model = MobileNetV2(input_shape=(128, 128, 3), include_top=False, weights=None)\n\nif os.path.exists(weights_path):\n    base_model.load_weights(weights_path)\nelse:\n    raise FileNotFoundError(f\"Weights file not found at {weights_path}\")\n\nfor layer in base_model.layers[:-20]:\n    layer.trainable = False\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Callbacks\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n]\n\n# Model Training\ntry:\n    history = model.fit(\n        train_generator,\n        epochs=30,\n        validation_data=validation_generator,\n        steps_per_epoch=train_generator.samples // 32,\n        validation_steps=validation_generator.samples // 32,\n        class_weight=class_weights,\n        callbacks=callbacks\n    )\nexcept KeyboardInterrupt:\n    print(\"Training interrupted. Saving model...\")\n    model.save('pixelation_detector_interrupted.h5')\n    raise\n\n# Evaluation\nval_preds = model.predict(validation_generator)\nval_labels = validation_generator.classes\nval_preds_binary = (val_preds > 0.5).astype(int).flatten()\n\n# Print classification report and confusion matrix\nprint(classification_report(val_labels, val_preds_binary))\nprint(confusion_matrix(val_labels, val_preds_binary))\n\n# Calculate F1 score\nf1 = f1_score(val_labels, val_preds_binary)\nprint(f\"F1 Score: {f1:.2f}\")\n\n# Precision-Recall Curve\nprecision, recall, thresholds = precision_recall_curve(val_labels, val_preds)\nprint(\"Precision-Recall curve calculated.\")\n\n# Calculate accuracy\naccuracy = accuracy_score(val_labels, val_preds_binary) * 100\nprint(f\"Accuracy: {accuracy:.2f}%\")\n\n# Save and check model size\nmodel_path = 'pixelation_detector.h5'\nmodel.save(model_path)\nmodel_size = os.path.getsize(model_path) / (1024 * 1024)  # Convert to MB\nprint(f\"Model size: {model_size:.2f} MB\")\n\n# Usage Example\ndef load_and_predict(image_path, model_path='pixelation_detector.h5'):\n    model = load_model(model_path)\n    image = cv2.imread(image_path)\n    image = cv2.resize(image, (128, 128))\n    image = image / 255.0  # Rescale\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n    prediction = model.predict(image)\n    is_pixelated = prediction[0][0] > 0.5\n    return is_pixelated\n\n# Example usage\nimage_path = '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing/Original/120.png'\nis_pixelated = load_and_predict(image_path)\nprint(f\"The image is {'pixelated' if is_pixelated else 'not pixelated'}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T16:04:02.900055Z","iopub.execute_input":"2024-06-26T16:04:02.900497Z","iopub.status.idle":"2024-06-26T16:09:17.457844Z","shell.execute_reply.started":"2024-06-26T16:04:02.900462Z","shell.execute_reply":"2024-06-26T16:09:17.456499Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Found 1992 images belonging to 2 classes.\nFound 498 images belonging to 2 classes.\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - accuracy: 0.5243 - loss: 0.7895 - val_accuracy: 0.5437 - val_loss: 0.7211 - learning_rate: 1.0000e-04\nEpoch 2/30\n\u001b[1m 1/62\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 302ms/step - accuracy: 0.6250 - loss: 0.6062","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6250 - loss: 0.6062 - val_accuracy: 0.6667 - val_loss: 0.5967 - learning_rate: 1.0000e-04\nEpoch 3/30\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 996ms/step - accuracy: 0.6261 - loss: 0.6565 - val_accuracy: 0.5500 - val_loss: 0.7442 - learning_rate: 1.0000e-04\nEpoch 4/30\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6562 - loss: 0.5091 - val_accuracy: 0.6111 - val_loss: 0.7182 - learning_rate: 1.0000e-04\nEpoch 5/30\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - accuracy: 0.6298 - loss: 0.6156 - val_accuracy: 0.5667 - val_loss: 0.8492 - learning_rate: 1.0000e-04\nEpoch 6/30\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6250 - loss: 0.7091 - val_accuracy: 0.6111 - val_loss: 0.7345 - learning_rate: 2.0000e-05\nEpoch 7/30\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - accuracy: 0.6316 - loss: 0.6485 - val_accuracy: 0.5396 - val_loss: 0.8589 - learning_rate: 2.0000e-05\n\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step\n              precision    recall  f1-score   support\n\n           0       0.53      0.31      0.39       249\n           1       0.51      0.73      0.60       249\n\n    accuracy                           0.52       498\n   macro avg       0.52      0.52      0.50       498\nweighted avg       0.52      0.52      0.50       498\n\n[[ 76 173]\n [ 67 182]]\nF1 Score: 0.60\nPrecision-Recall curve calculated.\nAccuracy: 51.81%\nModel size: 22.10 MB\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\nThe image is pixelated\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define constants\nIMG_HEIGHT, IMG_WIDTH = 128, 128  # Input image dimensions after downsampling\nBATCH_SIZE = 32\nEPOCHS = 1\n\n# Define paths to your dataset folders\ntrain_dir = '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing/'\ntrain_pixelated_dir = train_dir + 'Pixelated'\ntrain_original_dir = train_dir + 'Original'\n\n# ImageDataGenerator for augmentation and scaling\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2  # Splitting the data for validation\n)\n\n# Generate batches of augmented data\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training'  # Specify subset for training\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation'  # Specify subset for validation\n)\n\n# Define the model architecture\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n    MaxPooling2D((2, 2)),\n    BatchNormalization(),\n    \n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    BatchNormalization(),\n    \n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    BatchNormalization(),\n    \n    GlobalAveragePooling2D(),\n    \n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    \n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(),\n              loss=BinaryCrossentropy(),\n              metrics=[BinaryAccuracy(), Precision(), Recall()])\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // BATCH_SIZE\n)\n\n# Save the model\nmodel.save('/kaggle/working/pixelated_detection_model.h5')\n\n# Calculate and print model size\nimport os\nmodel_size = os.path.getsize('/kaggle/working/pixelated_detection_model.h5') / (1024 * 1024)  # in MB\nprint(f'Model Size: {model_size:.2f} MB')\n\n# Evaluate the model\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_directory(\n    '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing',\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False  # Ensure to evaluate in order\n)\n\nevaluation_results = model.evaluate(test_generator)\nloss, accuracy, precision, recall = evaluation_results[:4]\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')\n\n# Example of using the trained model for prediction\nsample_image_path = '/kaggle/input/trials/trial/original.jpg'\nsample_image = tf.keras.preprocessing.image.load_img(sample_image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\nsample_image = tf.keras.preprocessing.image.img_to_array(sample_image)\nsample_image = np.expand_dims(sample_image, axis=0) / 255.0  # Rescale and expand dimensions\nprediction = model.predict(sample_image)\nif prediction[0] > 0.5:\n    print('Prediction: Pixelated')\nelse:\n    print('Prediction: Not Pixelated')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T13:43:32.314812Z","iopub.execute_input":"2024-06-29T13:43:32.315288Z","iopub.status.idle":"2024-06-29T13:46:00.573871Z","shell.execute_reply.started":"2024-06-29T13:43:32.315223Z","shell.execute_reply":"2024-06-29T13:46:00.572647Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Found 1992 images belonging to 2 classes.\nFound 498 images belonging to 2 classes.\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - binary_accuracy: 0.5381 - loss: 0.7787 - precision_8: 0.5493 - recall_8: 0.4367 - val_binary_accuracy: 0.5146 - val_loss: 0.6923 - val_precision_8: 0.5676 - val_recall_8: 0.0882\nModel Size: 1.24 MB\nFound 2490 images belonging to 2 classes.\n\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 686ms/step - binary_accuracy: 0.7916 - loss: 0.6328 - precision_8: 0.1582 - recall_8: 0.0369\nTest Accuracy: 50.20%\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\nPrediction: Not Pixelated\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Define constants\nIMG_HEIGHT, IMG_WIDTH = 128, 128  # Input image dimensions after downsampling\nBATCH_SIZE = 32\nEPOCHS = 50\n\n# Define paths to your dataset folders\ntrain_dir = '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing/'\ntrain_pixelated_dir = train_dir + 'Pixelated'\ntrain_original_dir = train_dir + 'Original'\n\n# ImageDataGenerator for augmentation and scaling\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2  # Splitting the data for validation\n)\n\n# Generate batches of augmented data\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training'  # Specify subset for training\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation'  # Specify subset for validation\n)\n\n# Define the model architecture\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n    MaxPooling2D((2, 2)),\n    BatchNormalization(),\n\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    BatchNormalization(),\n\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    BatchNormalization(),\n\n    Conv2D(256, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    BatchNormalization(),\n\n    GlobalAveragePooling2D(),\n\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss=BinaryCrossentropy(),\n              metrics=[BinaryAccuracy(), Precision(), Recall()])\n\n# Define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\nmodel_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n)\n\n# Save the model\nmodel.save('/kaggle/working/pixelated_detection_model.keras')\n\n# Calculate and print model size\nimport os\nmodel_size = os.path.getsize('/kaggle/working/pixelated_detection_model.keras') / (1024 * 1024)  # in MB\nprint(f'Model Size: {model_size:.2f} MB')\n\n# Evaluate the model\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_directory(\n    '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing',\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False  # Ensure to evaluate in order\n)\n\nevaluation_results = model.evaluate(test_generator)\nloss, accuracy, precision, recall = evaluation_results[:4]\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')\n\n# Example of using the trained model for prediction\nsample_image_path = '/kaggle/input/trials/trial/original.jpg'\nsample_image = tf.keras.preprocessing.image.load_img(sample_image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\nsample_image = tf.keras.preprocessing.image.img_to_array(sample_image)\nsample_image = np.expand_dims(sample_image, axis=0) / 255.0  # Rescale and expand dimensions\nprediction = model.predict(sample_image)\nif prediction[0] > 0.5:\n    print('Prediction: Pixelated')\nelse:\n    print('Prediction: Not Pixelated')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T14:19:24.083716Z","iopub.execute_input":"2024-06-29T14:19:24.084159Z","iopub.status.idle":"2024-06-29T14:21:56.067283Z","shell.execute_reply.started":"2024-06-29T14:19:24.084129Z","shell.execute_reply":"2024-06-29T14:21:56.066043Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Found 1992 images belonging to 2 classes.\nFound 498 images belonging to 2 classes.\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - binary_accuracy: 0.5149 - loss: 0.9154 - precision_12: 0.5149 - recall_12: 0.4640 - val_binary_accuracy: 0.5021 - val_loss: 0.7030 - val_precision_12: 0.5045 - val_recall_12: 0.2333 - learning_rate: 0.0010\nModel Size: 5.01 MB\n\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 689ms/step - binary_accuracy: 0.7320 - loss: 0.6440 - precision_12: 0.1702 - recall_12: 0.0934\nTest Accuracy: 49.72%\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\nPrediction: Not Pixelated\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Define constants\nIMG_HEIGHT, IMG_WIDTH = 128, 128\nBATCH_SIZE = 32\nEPOCHS = 10  # Increased number of epochs\n\n# Define paths to your dataset folders\ntrain_dir = '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing/'\ntrain_pixelated_dir = os.path.join(train_dir, 'Pixelated')\ntrain_original_dir = os.path.join(train_dir, 'Original')\n\n# Function to split data into train and validation directories\ndef split_data(original_dir, pixelated_dir, train_dir, test_dir, split_ratio):\n    os.makedirs(os.path.join(train_dir, 'Original'), exist_ok=True)\n    os.makedirs(os.path.join(train_dir, 'Pixelated'), exist_ok=True)\n    os.makedirs(os.path.join(test_dir, 'Original'), exist_ok=True)\n    os.makedirs(os.path.join(test_dir, 'Pixelated'), exist_ok=True)\n    \n    original_files = os.listdir(original_dir)\n    pixelated_files = os.listdir(pixelated_dir)\n    paired_files = [(file, file) for file in original_files if file in pixelated_files]\n    np.random.shuffle(paired_files)\n    split_index = int(len(paired_files) * split_ratio)\n    train_files = paired_files[:split_index]\n    test_files = paired_files[split_index:]\n    \n    for original_file, pixelated_file in train_files:\n        shutil.copy(os.path.join(original_dir, original_file), os.path.join(train_dir, 'Original', original_file))\n        shutil.copy(os.path.join(pixelated_dir, pixelated_file), os.path.join(train_dir, 'Pixelated', pixelated_file))\n        \n    for original_file, pixelated_file in test_files:\n        shutil.copy(os.path.join(original_dir, original_file), os.path.join(test_dir, 'Original', original_file))\n        shutil.copy(os.path.join(pixelated_dir, pixelated_file), os.path.join(test_dir, 'Pixelated', pixelated_file))\n\n# Split the data\nbase_dir = '/kaggle/working/'\ntrain_dir = os.path.join(base_dir, 'train')\ntest_dir = os.path.join(base_dir, 'test')\nsplit_ratio = 0.8\nsplit_data(train_original_dir, train_pixelated_dir, train_dir, test_dir, split_ratio)\n\n# ImageDataGenerator for augmentation and scaling\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,  # Reduced range\n    width_shift_range=0.1,  # Reduced range\n    height_shift_range=0.1,  # Reduced range\n    shear_range=0.1,  # Reduced range\n    zoom_range=0.1,  # Reduced range\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\n# Generate batches of augmented data\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    test_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary'\n)\n\n# Define the model architecture\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n    MaxPooling2D((2, 2)),\n    # BatchNormalization(),  # Removed for simplicity\n\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    # BatchNormalization(),  # Removed for simplicity\n\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    # BatchNormalization(),  # Removed for simplicity\n\n    Conv2D(256, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    # BatchNormalization(),  # Removed for simplicity\n\n    GlobalAveragePooling2D(),\n\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss=BinaryCrossentropy(),\n              metrics=[BinaryAccuracy(), Precision(), Recall()])\n\n# Define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # Increased patience\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\nmodel_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    callbacks=[early_stopping, reduce_lr, model_checkpoint]\n)\n\n# Save the model\nmodel.save('/kaggle/working/pixelated_detection_model.keras')\n\n# Calculate and print model size\nmodel_size = os.path.getsize('/kaggle/working/pixelated_detection_model.keras') / (1024 * 1024)\nprint(f'Model Size: {model_size:.2f} MB')\n\n# Evaluate the model\ntest_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False\n)\n\nevaluation_results = model.evaluate(test_generator)\nloss, accuracy, precision, recall = evaluation_results[:4]\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')\n\n# Example of using the trained model for prediction\nsample_image_path = '/kaggle/input/pixelated-image-detection-and-correction/Image_Processing/Original/102.png'\nsample_image = load_img(sample_image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\nsample_image = img_to_array(sample_image)\nsample_image = np.expand_dims(sample_image, axis=0) / 255.0\nprediction = model.predict(sample_image)\nif prediction[0] > 0.5:\n    print('Prediction: Pixelated')\nelse:\n    print('Prediction: Not Pixelated')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T16:15:17.924677Z","iopub.execute_input":"2024-07-03T16:15:17.925450Z","iopub.status.idle":"2024-07-03T16:24:16.429768Z","shell.execute_reply.started":"2024-07-03T16:15:17.925415Z","shell.execute_reply":"2024-07-03T16:24:16.428625Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 2390 images belonging to 2 classes.\nFound 898 images belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - binary_accuracy: 0.5663 - loss: 0.6710 - precision_1: 0.5661 - recall_1: 0.6550 - val_binary_accuracy: 0.6942 - val_loss: 0.6055 - val_precision_1: 0.8692 - val_recall_1: 0.4588 - learning_rate: 0.0010\nEpoch 4/10\n\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - binary_accuracy: 0.6875 - loss: 0.6187 - precision_1: 0.7778 - recall_1: 0.4667 - val_binary_accuracy: 1.0000 - val_loss: 0.5472 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\nEpoch 5/10\n\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - binary_accuracy: 0.6954 - loss: 0.5919 - precision_1: 0.8810 - recall_1: 0.4806 - val_binary_accuracy: 0.6998 - val_loss: 0.5721 - val_precision_1: 0.9279 - val_recall_1: 0.4318 - learning_rate: 0.0010\nEpoch 6/10\n\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - binary_accuracy: 0.6250 - loss: 0.6596 - precision_1: 0.8750 - recall_1: 0.3889 - val_binary_accuracy: 0.5000 - val_loss: 0.5301 - val_precision_1: 1.0000 - val_recall_1: 0.5000 - learning_rate: 0.0010\nEpoch 7/10\n\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - binary_accuracy: 0.6905 - loss: 0.5774 - precision_1: 0.8715 - recall_1: 0.4572 - val_binary_accuracy: 0.7009 - val_loss: 0.5510 - val_precision_1: 0.9455 - val_recall_1: 0.4263 - learning_rate: 0.0010\nEpoch 8/10\n\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 252ms/step - binary_accuracy: 0.6875 - loss: 0.5468 - precision_1: 1.0000 - recall_1: 0.4118 - val_binary_accuracy: 0.5000 - val_loss: 0.7286 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\nEpoch 9/10\n\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - binary_accuracy: 0.7180 - loss: 0.5429 - precision_1: 0.9250 - recall_1: 0.4773 - val_binary_accuracy: 0.7009 - val_loss: 0.5523 - val_precision_1: 0.9171 - val_recall_1: 0.4432 - learning_rate: 0.0010\nEpoch 10/10\n\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - binary_accuracy: 0.7500 - loss: 0.4103 - precision_1: 1.0000 - recall_1: 0.6364 - val_binary_accuracy: 1.0000 - val_loss: 0.4880 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - learning_rate: 0.0010\nModel Size: 4.97 MB\nFound 898 images belonging to 2 classes.\n\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 559ms/step - binary_accuracy: 0.8771 - loss: 0.5300 - precision_1: 0.4212 - recall_1: 0.2322\nTest Accuracy: 70.27%\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\nPrediction: Not Pixelated\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}